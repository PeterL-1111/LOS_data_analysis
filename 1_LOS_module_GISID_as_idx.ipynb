{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import statistics\n",
    "from scipy.optimize import curve_fit\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from random import sample\n",
    "import warnings\n",
    "from termcolor import colored\n",
    "import plotly.express as px\n",
    "import plotly.offline as pyo\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import gc\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "pd.set_option('max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Analyzing Speed Data: INRIX + Traffic Counter Data\n",
    "## Comprehensive Analysis of Traffic Speed Data for the State of Virginia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Counter Speed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Read the common file only once\n",
    "is_routes_tag_attr_df = pd.read_csv('test_tms_speed\\\\IS_routes_tag_attr.csv')\n",
    "\n",
    "# Extract unique and non-NA LINKIDs\n",
    "Study_area_LINKID = is_routes_tag_attr_df[['LINKID']].drop_duplicates().dropna()\n",
    "\n",
    "# Get all segment attribute\n",
    "Seg_attr = is_routes_tag_attr_df.copy()\n",
    "\n",
    "# Read the LINKID_continuous file\n",
    "LINKID_continuous = pd.read_csv('test_tms_speed\\\\speed_counter_data\\\\SearchLinkIDs.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Speed Counter Data transformation and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Daily_progress\\Speed_LOS_VDF\\LOS_module\\test_tms_speed\\speed_counter_data\\speeddata_0214_datapull.csv\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd ## Import pandas library\n",
    "\n",
    "rootdir = os.getcwd() ## Get current working directory\n",
    "list_output = [] ## Create empty list called list_output\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        if file.endswith(\"_datapull.csv\"): ## Check if file name ends with \"_datapull.csv\"\n",
    "            print(os.path.join(subdir, file)) ## Print file path\n",
    "            list_output.append(os.path.join(subdir, file)) ## Add file path to list_output\n",
    "\n",
    "result = pd.DataFrame() ## Create empty pandas DataFrame called result\n",
    "for file in list_output:\n",
    "    for chunk in pd.read_csv(file, names=[\"LINKID\", \"DateTime\", \"Dir\", \"CounterNumber\", \"Lane\", \"MPH_0_15\", \"MPH_15_25\", \"MPH_25_30\", \"MPH_30_35\", \"MPH_35_40\", \"MPH_40_45\", \"MPH_45_50\", \"MPH_50_55\", \"MPH_55_60\", \"MPH_60_65\", \"MPH_65_70\", \"MPH_70_75\", \"MPH_75_80\", \"MPH_80_85\", \"MPH_85_UP\", \"TotalVol\",\"VolQual\"],dtype={'CounterNumber':'object','Lane':'object'}, chunksize=5000000): \n",
    "        ## Read file as pandas DataFrame with specified column names and data types, and chunksize of 5000000 rows\n",
    "        result = pd.concat([result, chunk], ignore_index=True) ## Concatenate the chunks into result DataFrame\n",
    "\n",
    "result = result.drop_duplicates() ## Drop duplicate rows from the result DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Convert the 'DateTime' column to datetime format, inferring the format automatically.\n",
    "result['DateTime_formatted'] = pd.to_datetime(result['DateTime'], infer_datetime_format=True)\n",
    "\n",
    "# Extract the year from the 'DateTime_formatted' and store it in a new 'Year' column.\n",
    "result['Year'] = result['DateTime_formatted'].dt.year\n",
    "\n",
    "# Extract the month from the 'DateTime_formatted' and store it in a new 'Mon' (Month) column.\n",
    "result['Mon'] = result['DateTime_formatted'].dt.month\n",
    "\n",
    "# Extract the day of the month from the 'DateTime_formatted' and store it in a new 'Day' column.\n",
    "result['Day'] = result['DateTime_formatted'].dt.day\n",
    "\n",
    "# Extract the hour from the 'DateTime_formatted' and store it in a new 'Hour' column.\n",
    "result['Hour'] = result['DateTime_formatted'].dt.hour\n",
    "\n",
    "# Extract the day of the week as an integer (0 for Monday, 6 for Sunday) and store it in a new 'Wkday' column.\n",
    "result['Wkday'] = result['DateTime_formatted'].dt.dayofweek\n",
    "\n",
    "# Create a 'Date' column by combining year, month, and day, and converting it to an integer format (YYYYMMDD).\n",
    "result['Date'] = (result['DateTime_formatted'].dt.year * 10000 + result['DateTime_formatted'].dt.month * 100 + result['DateTime_formatted'].dt.day).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data coverage from continuous count table\n",
      "LinkIDs in LINKID_continuous but not in result: [50154, 120025, 150156, 782797, 782834, 782838]\n"
     ]
    }
   ],
   "source": [
    "# Indicate that there is no data coverage from the continuous count table.\n",
    "print(\"No data coverage from continuous count table\")\n",
    "\n",
    "# Calculate the set difference using Pandas built-in methods for better efficiency.\n",
    "difference = set(LINKID_continuous['LinkID'].unique()) - set(result['LINKID'].unique())\n",
    "\n",
    "# Sort the set for better readability and then print it.\n",
    "print(f\"LinkIDs in LINKID_continuous but not in result: {sorted(difference)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINKIDs that do not have all coverage from 2017 - 2021\n",
      "Incomplete coverage for LINKIDs: [10005, 10148, 10181, 10185, 40042, 40056, 40099, 40412, 40489, 40551, 40648, 40765, 50119, 50149, 120013, 140013, 140015, 140026, 140031, 140048, 150036, 150037, 150038, 160105, 180005, 190029, 199774, 781288, 781963, 791023]\n"
     ]
    }
   ],
   "source": [
    "# Print the message to indicate the purpose of the output\n",
    "print(\"LINKIDs that do not have all coverage from 2017 - 2021\")\n",
    "\n",
    "# Perform the groupby and filtering operation\n",
    "grouped_result = result.groupby([\"LINKID\", \"Year\"]).size().reset_index(name='TotalVol')\n",
    "filtered_result = grouped_result.groupby('LINKID').filter(lambda x: len(x) < 5)\n",
    "\n",
    "# Extract unique LINKIDs and convert them to a set for efficient removal\n",
    "list_coverage_missing = set(filtered_result['LINKID'].unique())\n",
    "\n",
    "# Remove the specific LINKIDs 190073 and 199750\n",
    "list_coverage_missing -= {190073, 199750}\n",
    "\n",
    "# Convert back to list and sort for readability\n",
    "list_coverage_missing = sorted(list_coverage_missing)\n",
    "\n",
    "# Print the list of remaining LINKIDs with incomplete coverage\n",
    "print(f\"Incomplete coverage for LINKIDs: {list_coverage_missing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# remove the LINKIDs that does not have full year  coverage\n",
    "result = result[~result.LINKID.isin(list_coverage_missing)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Requested LINKIDs: 1423\n",
      "Total available LINKIDs from Speed Counter: 453\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Requested_LINKID</th>\n",
       "      <th>missing_data_LINKID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90112.0</td>\n",
       "      <td>90112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90114.0</td>\n",
       "      <td>90114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90116.0</td>\n",
       "      <td>90116.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90118.0</td>\n",
       "      <td>90118.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90120.0</td>\n",
       "      <td>90120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>90051.0</td>\n",
       "      <td>90051.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>980188.0</td>\n",
       "      <td>980188.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>90106.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>90108.0</td>\n",
       "      <td>90108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422</th>\n",
       "      <td>90110.0</td>\n",
       "      <td>90110.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1423 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Requested_LINKID  missing_data_LINKID\n",
       "0              90112.0              90112.0\n",
       "1              90114.0              90114.0\n",
       "2              90116.0              90116.0\n",
       "3              90118.0              90118.0\n",
       "4              90120.0              90120.0\n",
       "...                ...                  ...\n",
       "1418           90051.0              90051.0\n",
       "1419          980188.0             980188.0\n",
       "1420           90106.0                  NaN\n",
       "1421           90108.0              90108.0\n",
       "1422           90110.0              90110.0\n",
       "\n",
       "[1423 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate unique LINKIDs only once for efficiency\n",
    "unique_study_area_LINKID = set(Study_area_LINKID['LINKID'].unique())\n",
    "unique_result_LINKID = set(result['LINKID'].unique())\n",
    "\n",
    "# Get unique LINKIDs from Study_area_LINKID that are not in result\n",
    "unique_in_request_list = unique_study_area_LINKID - unique_result_LINKID\n",
    "\n",
    "# Create a DataFrame of unique LINKIDs from Study_area_LINKID\n",
    "summary_df = pd.DataFrame({'Requested_LINKID': list(unique_study_area_LINKID)})\n",
    "\n",
    "# Merge with DataFrame of missing LINKIDs using a left join\n",
    "summary_df = pd.merge(\n",
    "    summary_df, \n",
    "    pd.DataFrame({'missing_data_LINKID': list(unique_in_request_list)}),  \n",
    "    how='left', \n",
    "    left_on=['Requested_LINKID'], \n",
    "    right_on=['missing_data_LINKID']\n",
    ")\n",
    "\n",
    "# Print summary information\n",
    "print(f\"Total Requested LINKIDs: {summary_df['Requested_LINKID'].nunique()}\")\n",
    "print(f\"Total available LINKIDs from Speed Counter: {summary_df['missing_data_LINKID'].isna().sum()}\")\n",
    "\n",
    "# Show the summary DataFrame\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Save the unique LINKIDs from the result DataFrame to a CSV file\n",
    "pd.DataFrame({'Speed_counter_Available_LINKID': list(result['LINKID'].unique())}).to_csv('output\\\\Available_speed_counter_LINKID.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Save the unique missing_data_LINKID values from the summary_df DataFrame to a CSV file\n",
    "pd.DataFrame({'missing_data_LINKID': list(summary_df['missing_data_LINKID'].dropna().unique())}).to_csv('output\\\\missing_speed_counter_LINKID.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed counter data structure\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LINKID</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Dir</th>\n",
       "      <th>CounterNumber</th>\n",
       "      <th>Lane</th>\n",
       "      <th>MPH_0_15</th>\n",
       "      <th>MPH_15_25</th>\n",
       "      <th>MPH_25_30</th>\n",
       "      <th>MPH_30_35</th>\n",
       "      <th>MPH_35_40</th>\n",
       "      <th>MPH_40_45</th>\n",
       "      <th>MPH_45_50</th>\n",
       "      <th>MPH_50_55</th>\n",
       "      <th>MPH_55_60</th>\n",
       "      <th>MPH_60_65</th>\n",
       "      <th>MPH_65_70</th>\n",
       "      <th>MPH_70_75</th>\n",
       "      <th>MPH_75_80</th>\n",
       "      <th>MPH_80_85</th>\n",
       "      <th>MPH_85_UP</th>\n",
       "      <th>TotalVol</th>\n",
       "      <th>VolQual</th>\n",
       "      <th>DateTime_formatted</th>\n",
       "      <th>Year</th>\n",
       "      <th>Mon</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Wkday</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40301</td>\n",
       "      <td>01/01/2017 00:00</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>20170101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40301</td>\n",
       "      <td>01/01/2017 00:15</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>69</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-01-01 00:15:00</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>20170101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40301</td>\n",
       "      <td>01/01/2017 00:30</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>46</td>\n",
       "      <td>70</td>\n",
       "      <td>29</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>178</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-01-01 00:30:00</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>20170101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40301</td>\n",
       "      <td>01/01/2017 00:45</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>44</td>\n",
       "      <td>50</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-01-01 00:45:00</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>20170101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40301</td>\n",
       "      <td>01/01/2017 01:00</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>38</td>\n",
       "      <td>46</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-01-01 01:00:00</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>20170101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LINKID          DateTime Dir CounterNumber Lane  MPH_0_15  MPH_15_25  \\\n",
       "0   40301  01/01/2017 00:00   N             1    3         0          0   \n",
       "1   40301  01/01/2017 00:15   N             1    3         0          0   \n",
       "2   40301  01/01/2017 00:30   N             1    3         0          1   \n",
       "3   40301  01/01/2017 00:45   N             1    3         0          0   \n",
       "4   40301  01/01/2017 01:00   N             1    3         0          0   \n",
       "\n",
       "   MPH_25_30  MPH_30_35  MPH_35_40  MPH_40_45  MPH_45_50  MPH_50_55  \\\n",
       "0          0          0          3          2         17         39   \n",
       "1          0          0          0          7         17         69   \n",
       "2          0          0          0         15         46         70   \n",
       "3          0          0          4         19         44         50   \n",
       "4          0          0          0          9         38         46   \n",
       "\n",
       "   MPH_55_60  MPH_60_65  MPH_65_70  MPH_70_75  MPH_75_80  MPH_80_85  \\\n",
       "0         15          9          2          0          0          0   \n",
       "1         36          4          2          0          0          0   \n",
       "2         29         15          2          0          0          0   \n",
       "3         31          4          4          0          0          0   \n",
       "4         28          4          1          0          0          0   \n",
       "\n",
       "   MPH_85_UP  TotalVol  VolQual  DateTime_formatted  Year  Mon  Day  Hour  \\\n",
       "0          0        87        4 2017-01-01 00:00:00  2017    1    1     0   \n",
       "1          0       135        4 2017-01-01 00:15:00  2017    1    1     0   \n",
       "2          0       178        4 2017-01-01 00:30:00  2017    1    1     0   \n",
       "3          0       156        4 2017-01-01 00:45:00  2017    1    1     0   \n",
       "4          0       126        4 2017-01-01 01:00:00  2017    1    1     1   \n",
       "\n",
       "   Wkday      Date  \n",
       "0      6  20170101  \n",
       "1      6  20170101  \n",
       "2      6  20170101  \n",
       "3      6  20170101  \n",
       "4      6  20170101  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Speed counter data structure\")\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Calculate Average speed from the volume and speed brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# calculate the hourly aggrgate of vol/spd summary\n",
    "grouped = result.groupby([\"LINKID\",\"Dir\",\"Date\",\"Hour\"])\n",
    "df_vol_spd = grouped.agg({'MPH_0_15': 'sum', \n",
    "                     'MPH_15_25': 'sum',\n",
    "                     'MPH_25_30': 'sum', \n",
    "                     'MPH_30_35': 'sum',\n",
    "                     'MPH_35_40': 'sum',\n",
    "                     'MPH_40_45': 'sum', \n",
    "                     'MPH_45_50': 'sum', \n",
    "                     'MPH_50_55': 'sum',\n",
    "                     'MPH_55_60': 'sum',\n",
    "                     'MPH_60_65': 'sum',\n",
    "                     'MPH_65_70': 'sum',\n",
    "                     'MPH_70_75': 'sum',\n",
    "                     'MPH_75_80': 'sum',\n",
    "                     'MPH_80_85': 'sum',\n",
    "                     'MPH_85_UP': 'sum',\n",
    "                     'TotalVol': 'sum',\n",
    "                     'VolQual': 'mean',\n",
    "                     'Year': 'mean',\n",
    "                     'Mon': 'mean',\n",
    "                     'Day': 'mean',\n",
    "                     'Wkday': 'mean'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Delete individual variables to free up memory\n",
    "del chunk, grouped, result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Calculate the average speed from the speed data and add it as a new column called 'AvgSpd'\n",
    "df_vol_spd['AvgSpd'] = (df_vol_spd['MPH_0_15']*7.5+ df_vol_spd['MPH_15_25']*20+ \n",
    "                        df_vol_spd['MPH_25_30']*27.5+df_vol_spd['MPH_30_35']*32.5+ \n",
    "                        df_vol_spd['MPH_35_40']*37.5+df_vol_spd['MPH_40_45']*42.5+ \n",
    "                        df_vol_spd['MPH_45_50']*47.5+df_vol_spd['MPH_50_55']*52.5+ \n",
    "                        df_vol_spd['MPH_55_60']*57.5+df_vol_spd['MPH_60_65']*62.5+ \n",
    "                        df_vol_spd['MPH_65_70']*67.5+df_vol_spd['MPH_70_75']*72.5+ \n",
    "                        df_vol_spd['MPH_75_80']*77.5+df_vol_spd['MPH_80_85']*82.5+ \n",
    "                        df_vol_spd['MPH_85_UP']*85)/df_vol_spd['TotalVol']\n",
    "\n",
    "## Add a new column called 'Source' and set the value to 'Speed Counter'\n",
    "df_vol_spd['Source'] = \"Speed Counter\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Data cleaning and Reasonableness check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Filter out rows where AvgSpd is null and reset the DataFrame index\n",
    "df_vol_spd = df_vol_spd[df_vol_spd['AvgSpd'].notnull()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique LINKIDs where average speed is 7.5 mph: [10332, 10397, 10398, 10399, 10443, 10595, 20145, 20172, 20178, 20180, 20233, 20234, 20350, 40001, 40040, 40114, 40300, 40359, 40413, 40500, 40502, 40506, 50239, 50267, 60164, 60170, 80232, 80238, 80244, 80299, 80370, 80372, 80422, 80427, 80460, 80502, 90106, 90139, 90141, 90305, 110011, 110029, 120012, 120015, 120016, 120027, 120028, 140039, 140052, 140057, 140072, 140077, 140312, 150112, 150113, 150319, 180001, 180008, 180011, 180016, 180019, 180022, 180024, 180035, 190013, 190015, 199741, 199750, 199760, 781003, 781100, 781108, 781220, 781223, 781229, 781230, 781249, 781984, 781985, 786964, 787049, 787120, 787169, 787171, 789282, 789288, 789299, 789347, 789359, 789360, 789362, 789363, 789409, 789420, 789443, 789446, 789811, 789822, 790816, 792625, 795433, 880012, 880032, 920380, 940419, 980503]\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame for rows where AvgSpd is 7.5, group them by LINKID, and then extract unique LINKIDs\n",
    "unique_linkid_list = df_vol_spd.query('AvgSpd == 7.5').groupby('LINKID').size().index.tolist()\n",
    "\n",
    "# Print the list of unique LINKIDs that meet the condition\n",
    "print(\"Unique LINKIDs where average speed is 7.5 mph:\", unique_linkid_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_vol_spd = df_vol_spd.query('AvgSpd > 7.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Select specific columns from df_vol_spd and drop any duplicate rows based on those columns\n",
    "df_vol_spd = df_vol_spd[[\"LINKID\", \"Date\", \"Hour\", \"Year\", \"Mon\", \"Day\", \"Wkday\", \"TotalVol\", \"AvgSpd\", \"Source\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# remove the LINKIDs that does not have full year  coverage\n",
    "df_vol_spd = df_vol_spd[~df_vol_spd.LINKID.isin(list_coverage_missing)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Filter out LINKIDs that have data but not enough entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Group the data by LINKID and count the number of rows for each group, saving the results to 'grouped'\n",
    "grouped = df_vol_spd.groupby(['LINKID']).size().reset_index(name='counts')\n",
    "\n",
    "## Set a filter value and filter 'grouped' to only include rows where 'counts' is greater than or equal to 'filter_value'\n",
    "filter_value = 720\n",
    "filtered_grouped = grouped[grouped['counts']>=filter_value]\n",
    "\n",
    "## Extract the LINKIDs to keep based on 'filtered_grouped'\n",
    "LINKID_to_keep = filtered_grouped.LINKID\n",
    "\n",
    "## Filter the original DataFrame to only include rows where LINKID is in 'LINKID_to_keep'\n",
    "filtered_df_vol_spd = df_vol_spd[df_vol_spd.LINKID.isin(LINKID_to_keep)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed counter data that has less than 30 days coverage in a year\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LINKID</th>\n",
       "      <th>Year</th>\n",
       "      <th>TotalVol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [LINKID, Year, TotalVol]\n",
       "Index: []"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Speed counter data that has less than 30 days coverage in a year\")\n",
    "df_vol_spd.query('Source ==\"Speed Counter\"').groupby([\"LINKID\",\"Year\"]).agg({'TotalVol':np.size}).reset_index().query('TotalVol<720')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter: data entry more than 720 per LINKID\n",
      "number of LINKID before filter:  507\n",
      "number of LINKID after filter:  507\n"
     ]
    }
   ],
   "source": [
    "print(\"filter: data entry more than\", filter_value, \"per LINKID\")\n",
    "print(\"number of LINKID before filter: \", len(df_vol_spd.LINKID.unique()))\n",
    "print(\"number of LINKID after filter: \", len(filtered_df_vol_spd.LINKID.unique()))\n",
    "\n",
    "unique_in_result =  set(df_vol_spd.LINKID.unique()) - set(filtered_df_vol_spd.LINKID.unique())\n",
    "\n",
    "summary_df = pd.merge(summary_df, pd.DataFrame({'Not_enough_data_LINKID': list(unique_in_result)}),  how='left', left_on=['Requested_LINKID'], right_on =['Not_enough_data_LINKID'])\n",
    "#summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_vol_spd = filtered_df_vol_spd.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc  # Importing the garbage collector module\n",
    "\n",
    "# Delete individual DataFrames to free up memory\n",
    "del filtered_df_vol_spd\n",
    "del grouped\n",
    "del filtered_grouped\n",
    "\n",
    "# Call the garbage collector to free up memory\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total LINKIDs from Speed counter: 507\n"
     ]
    }
   ],
   "source": [
    "print(\"Total LINKIDs from Speed counter:\", len(df_vol_spd.query('Source == \"Speed Counter\"').LINKID.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Add on data from volumn data and INRIX conflation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Volume distribution profile to approximate hourly volume and INRIX hourly speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Create distribution based on upstream linkid and apply to the AADT\n",
    "## Group the data by LINKID, Year, and Hour, calculate the mean of TotalVol for each group, and save the result to 'df_AADT_dist'\n",
    "df_AADT_dist = df_vol_spd.groupby([\"LINKID\",\"Year\",\"Hour\"]).agg({'TotalVol':np.mean}).reset_index()\n",
    "\n",
    "## Rename the 'TotalVol' column to 'Hourly_Vol' in 'df_AADT_dist'\n",
    "df_AADT_dist = df_AADT_dist.rename(columns={\"TotalVol\": \"Hourly_Vol\"})\n",
    "\n",
    "## Group 'df_AADT_dist' by LINKID and Year, calculate the sum of Hourly_Vol for each group, and save the result to 'df_AADT_dist_sum'\n",
    "df_AADT_dist_sum = df_AADT_dist.groupby([\"LINKID\",\"Year\"]).agg({'Hourly_Vol':np.sum}).reset_index()\n",
    "\n",
    "## Rename the 'Hourly_Vol' column to 'Total_Daily_Vol' in 'df_AADT_dist_sum'\n",
    "df_AADT_dist_sum.rename(columns={\"Hourly_Vol\": \"Total_Daily_Vol\"}, inplace=True)\n",
    "\n",
    "## Merge 'df_AADT_dist' with 'df_AADT_dist_sum' based on the 'LINKID' and 'Year' columns\n",
    "df_AADT_dist = pd.merge(df_AADT_dist, df_AADT_dist_sum,  how='left', left_on=['LINKID','Year'], right_on =['LINKID','Year'])\n",
    "\n",
    "## Create a new column in 'df_AADT_dist' called 'hourly_dist' and calculate the ratio of Hourly_Vol to Total_Daily_Vol for each row\n",
    "df_AADT_dist['hourly_dist'] = df_AADT_dist['Hourly_Vol']/df_AADT_dist['Total_Daily_Vol'] \n",
    "df_AADT_dist = df_AADT_dist.rename(columns={\"LINKID\": \"LINKID_to_ref\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## obtain linkid ref from shapefile\n",
    "LINKID_ref = (Seg_attr[['LINKID', 'Refer_LINKID', 'INRIX_XDID']]\n",
    "              .dropna()\n",
    "              .sort_values('LINKID')\n",
    "              .rename(columns={\"Refer_LINKID\": \"LINKID_ref\", \"INRIX_XDID\": \"XDSEG\"})\n",
    "              .reset_index(drop=True))\n",
    "\n",
    "# Read the export_aadt.csv file into a DataFrame\n",
    "LINKID_aadt_addon = pd.read_csv('test_tms_speed\\\\volume_counter_data\\\\ADT_distribution_volume\\\\export_aadt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINKID_ref comes from the shapefile, selecting LINKID, Refer_LINKID, INRIX_XDID,\n",
      "    then remove duplicates, filter when Refer_LINKID is not null\n",
      "Total number of LINKID with reference: 779\n"
     ]
    }
   ],
   "source": [
    "# Print a message indicating the source and columns of LINKID_ref, and display the total number of unique LINKID with reference\n",
    "print(\"LINKID_ref comes from the shapefile, selecting LINKID, Refer_LINKID, INRIX_XDID,\\n    then remove duplicates, filter when Refer_LINKID is not null\")\n",
    "print(f\"Total number of LINKID with reference: {len(LINKID_ref['LINKID'].unique())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Join with INRIX hourly Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Convert the 'AADTYEAR' column to a datetime object and extract the year to a new 'Year' column\n",
    "LINKID_aadt_addon['Year'] = pd.to_datetime(LINKID_aadt_addon['AADTYEAR'], infer_datetime_format=True).dt.year\n",
    "\n",
    "# Merge LINKID_ref and LINKID_aadt_addon DataFrames based on the 'LINKID' column\n",
    "df_dist_LINKID_ref = pd.merge(LINKID_ref, LINKID_aadt_addon, how='left', on=['LINKID'])\n",
    "\n",
    "# Read the INRIX_spd_agg.csv file into a DataFrame\n",
    "df_spd_agg = pd.read_csv('test_tms_speed\\\\volume_counter_data\\\\ADT_distribution_volume\\\\INRIX_data\\\\INRIX_spd_agg.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Merge df_dist_LINKID_ref and df_AADT_dist DataFrames based on 'LINKID_ref' and 'Year'\n",
    "Dist_ADT = pd.merge(df_dist_LINKID_ref, df_AADT_dist, how='left', left_on=['LINKID_ref', 'Year'], right_on=['LINKID_to_ref', 'Year'])\n",
    "\n",
    "# Calculate 'TotalVol' as the product of 'AADT' and 'hourly_dist'\n",
    "Dist_ADT[\"TotalVol\"] = Dist_ADT[\"AADT\"] * Dist_ADT[\"hourly_dist\"]\n",
    "\n",
    "# Filter out rows where 'TotalVol' is null and reset the index\n",
    "Dist_ADT = (Dist_ADT[Dist_ADT['TotalVol'].notnull()]\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "# Merge Dist_ADT and df_spd_agg DataFrames based on 'XDSEG', 'Year', and 'Hour'\n",
    "Dist_ADT = pd.merge(Dist_ADT, df_spd_agg, how='left', on=['XDSEG', 'Year', 'Hour'])\n",
    "Dist_ADT = Dist_ADT.groupby([\"LINKID\",\"Year\",\"Hour\"]).agg({'TotalVol': 'mean','Spd': 'mean'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Pseudo date, day, month, and weekday columns and update data types\n",
    "Dist_ADT = Dist_ADT.assign(\n",
    "    Date=Dist_ADT['Year'] * 10000,\n",
    "    Day=99.0,\n",
    "    Mon=99.0,\n",
    "    Wkday=99.0\n",
    ").astype({\n",
    "    'Date': np.int64,\n",
    "    'TotalVol': np.int64\n",
    "})\n",
    "Dist_ADT = Dist_ADT.rename(columns={\"hourly_Vol_dist\": \"TotalVol\",\"Spd\":\"AvgSpd\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume year-hour distribution using upstream reference LINKID\n",
      "due to count data availability, it is recommended to use data from 2017\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LINKID</th>\n",
       "      <th>Year</th>\n",
       "      <th>Hour</th>\n",
       "      <th>TotalVol</th>\n",
       "      <th>AvgSpd</th>\n",
       "      <th>Date</th>\n",
       "      <th>Day</th>\n",
       "      <th>Mon</th>\n",
       "      <th>Wkday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10005.0</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>402</td>\n",
       "      <td>63.320442</td>\n",
       "      <td>20170000</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10005.0</td>\n",
       "      <td>2017</td>\n",
       "      <td>1.0</td>\n",
       "      <td>335</td>\n",
       "      <td>63.052055</td>\n",
       "      <td>20170000</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10005.0</td>\n",
       "      <td>2017</td>\n",
       "      <td>2.0</td>\n",
       "      <td>295</td>\n",
       "      <td>62.975000</td>\n",
       "      <td>20170000</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10005.0</td>\n",
       "      <td>2017</td>\n",
       "      <td>3.0</td>\n",
       "      <td>282</td>\n",
       "      <td>62.911846</td>\n",
       "      <td>20170000</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10005.0</td>\n",
       "      <td>2017</td>\n",
       "      <td>4.0</td>\n",
       "      <td>324</td>\n",
       "      <td>62.891667</td>\n",
       "      <td>20170000</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    LINKID  Year  Hour  TotalVol     AvgSpd      Date   Day   Mon  Wkday\n",
       "0  10005.0  2017   0.0       402  63.320442  20170000  99.0  99.0   99.0\n",
       "1  10005.0  2017   1.0       335  63.052055  20170000  99.0  99.0   99.0\n",
       "2  10005.0  2017   2.0       295  62.975000  20170000  99.0  99.0   99.0\n",
       "3  10005.0  2017   3.0       282  62.911846  20170000  99.0  99.0   99.0\n",
       "4  10005.0  2017   4.0       324  62.891667  20170000  99.0  99.0   99.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Volume year-hour distribution using upstream reference LINKID\")\n",
    "print(\"due to count data availability, it is recommended to use data from 2017\")\n",
    "Dist_ADT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total LINKIDs from AADT distribution and INRIX: 779\n",
      "total LINKIDs from speed counter 507\n"
     ]
    }
   ],
   "source": [
    "# Select specific columns, add a new 'Source' column, and count unique LINKIDs\n",
    "Dist_ADT = (Dist_ADT[[\"LINKID\", \"Date\", \"Hour\", \"Year\", \"Mon\", \"Day\", \"Wkday\", \"TotalVol\", \"AvgSpd\"]]\n",
    "            .assign(Source=\"Vol Distribution&INRIX Spd\"))\n",
    "Dist_ADT['Source'] = \"Vol Distribution&INRIX Spd\"\n",
    "print(\"Total LINKIDs from AADT distribution and INRIX:\", len(Dist_ADT.LINKID.unique()))\n",
    "print(\"total LINKIDs from speed counter\", len(df_vol_spd.query('Source == \"Speed Counter\"').LINKID.unique()))\n",
    "df_vol_spd = pd.concat([df_vol_spd,Dist_ADT], axis=0).reset_index()\n",
    "del df_vol_spd['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "IndexingError",
     "evalue": "Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32824/1925177649.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Merge Study_area_LINKID and df_vol_spd based on the 'LINKID' column and filter out rows where 'TotalVol' is null\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m df_vol_spd = (pd.merge(Study_area_LINKID, df_vol_spd, how='left', on=['LINKID'])\n\u001b[0m\u001b[0;32m      3\u001b[0m               \u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_vol_spd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TotalVol'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m               .reset_index(drop=True))\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\windows\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    930\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 931\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    932\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\windows\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1142\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_slice_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1144\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getbool_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1145\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\windows\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getbool_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m    946\u001b[0m         \u001b[1;31m# caller is responsible for ensuring non-None axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m         \u001b[0minds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\windows\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36mcheck_bool_indexer\u001b[1;34m(index, key)\u001b[0m\n\u001b[0;32m   2386\u001b[0m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2387\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2388\u001b[1;33m             raise IndexingError(\n\u001b[0m\u001b[0;32m   2389\u001b[0m                 \u001b[1;34m\"Unalignable boolean Series provided as \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2390\u001b[0m                 \u001b[1;34m\"indexer (index of the boolean Series and of \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexingError\u001b[0m: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match)."
     ]
    }
   ],
   "source": [
    "# Merge Study_area_LINKID and df_vol_spd based on 'LINKID'\n",
    "# Filter out rows where 'TotalVol' is null and reset the index\n",
    "df_vol_spd = pd.merge(Study_area_LINKID, df_vol_spd,  how='left', left_on=['LINKID'], right_on =['LINKID'])\n",
    "df_vol_spd = df_vol_spd[df_vol_spd.TotalVol.notnull()].reset_index()\n",
    "# Save the DataFrame to a CSV file without row numbers\n",
    "df_vol_spd.to_csv('output\\\\vol_spd.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Clean Road Attribute table and Join with df_vol_spd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Rename columns, filter out rows, select specific columns, and drop duplicates\n",
    "Seg_attr = (Seg_attr.rename(columns={\n",
    "                \"Lane _Capacity\": \"Lane_Capacity\",\n",
    "                \"Tag_POST_SPD\": \"POST_SPD\",\n",
    "                \"Lanes\": \"LANE_NUM\"\n",
    "            })\n",
    "            .query('LINKID > 0')\n",
    "            .loc[:, [\"ID\", \"LINKID\", \"ROUTE_NAME\", \"Lane_Capacity\", \"LANE_NUM\", \"POST_SPD\", \"urban\"]]\n",
    "            .drop_duplicates())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Extract the second to last character from the 'ROUTE_NAME' column and store it in a new 'Dir' column\n",
    "Seg_attr['Dir'] = Seg_attr['ROUTE_NAME'].str[-2:-1]\n",
    "# Calculate unique LINKIDs that are in Study_area_LINKID but not in Seg_attr\n",
    "unique_in_request_list_from_attr = set(Study_area_LINKID.LINKID.unique()) - set(Seg_attr.LINKID.unique())\n",
    "\n",
    "# Create a DataFrame from the list of missing LINKIDs and save it to a CSV file\n",
    "(pd.DataFrame({'missing_data_LINKID': list(unique_in_request_list_from_attr)})\n",
    " .to_csv('output\\\\missing_attribute.csv', index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     LINKID        Date  Hour    Year   Mon   Day  Wkday  TotalVol     AvgSpd  \\\n",
      "0  940761.0  20170000.0   0.0  2017.0  99.0  99.0   99.0     226.0  66.578275   \n",
      "1  940761.0  20170000.0   1.0  2017.0  99.0  99.0   99.0     171.0  66.415282   \n",
      "2  940761.0  20170000.0   2.0  2017.0  99.0  99.0   99.0     147.0  66.052632   \n",
      "3  940761.0  20170000.0   3.0  2017.0  99.0  99.0   99.0     151.0  66.101307   \n",
      "4  940761.0  20170000.0   4.0  2017.0  99.0  99.0   99.0     214.0  66.236364   \n",
      "\n",
      "                       Source  \n",
      "0  Vol Distribution&INRIX Spd  \n",
      "1  Vol Distribution&INRIX Spd  \n",
      "2  Vol Distribution&INRIX Spd  \n",
      "3  Vol Distribution&INRIX Spd  \n",
      "4  Vol Distribution&INRIX Spd  \n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of df_vol_spd DataFrame\n",
    "df_vol_spd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LINKID</th>\n",
       "      <th>ROUTE_NAME</th>\n",
       "      <th>Lane_Capacity</th>\n",
       "      <th>LANE_NUM</th>\n",
       "      <th>POST_SPD</th>\n",
       "      <th>urban</th>\n",
       "      <th>Dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>940761.0</td>\n",
       "      <td>R-VA   IS00295SB</td>\n",
       "      <td>2100</td>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>40046.0</td>\n",
       "      <td>R-VA   IS00295SB</td>\n",
       "      <td>1600</td>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>50218.0</td>\n",
       "      <td>R-VA   IS00264EB</td>\n",
       "      <td>1600</td>\n",
       "      <td>4</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>190077.0</td>\n",
       "      <td>R-VA   IS00395SB</td>\n",
       "      <td>1600</td>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50608.0</td>\n",
       "      <td>R-VA   IS00264EB</td>\n",
       "      <td>1400</td>\n",
       "      <td>3</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID    LINKID        ROUTE_NAME  Lane_Capacity  LANE_NUM  POST_SPD  urban  \\\n",
       "0   1  940761.0  R-VA   IS00295SB           2100         3        70      0   \n",
       "1   2   40046.0  R-VA   IS00295SB           1600         4        70      1   \n",
       "2   3   50218.0  R-VA   IS00264EB           1600         4        55      1   \n",
       "3   4  190077.0  R-VA   IS00395SB           1600         2        55      1   \n",
       "4   5   50608.0  R-VA   IS00264EB           1400         3        55      1   \n",
       "\n",
       "  Dir  \n",
       "0   S  \n",
       "1   S  \n",
       "2   E  \n",
       "3   S  \n",
       "4   E  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of Seg_attr DataFrame\n",
    "Seg_attr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Merge Seg_attr and df_vol_spd DataFrames based on 'LINKID' and drop rows with any NaN values\n",
    "df_joined = (pd.merge(Seg_attr, df_vol_spd, how='left', on=['LINKID'])\n",
    "             .dropna())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of LINKID with enough data: 1232\n",
      "Number of LINKID with enough data and Road Attributes: 1231\n",
      "Number of LINKID with enough data but missing Road Attributes: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Requested_LINKID</th>\n",
       "      <th>missing_data_LINKID</th>\n",
       "      <th>Not_enough_data_LINKID</th>\n",
       "      <th>missing_attribute_LINKID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90112.0</td>\n",
       "      <td>90112.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90114.0</td>\n",
       "      <td>90114.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90116.0</td>\n",
       "      <td>90116.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90118.0</td>\n",
       "      <td>90118.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90120.0</td>\n",
       "      <td>90120.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Requested_LINKID  missing_data_LINKID  Not_enough_data_LINKID  \\\n",
       "0           90112.0              90112.0                     NaN   \n",
       "1           90114.0              90114.0                     NaN   \n",
       "2           90116.0              90116.0                     NaN   \n",
       "3           90118.0              90118.0                     NaN   \n",
       "4           90120.0              90120.0                     NaN   \n",
       "\n",
       "   missing_attribute_LINKID  \n",
       "0                       NaN  \n",
       "1                       NaN  \n",
       "2                       NaN  \n",
       "3                       NaN  \n",
       "4                       NaN  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the number of unique LINKIDs in df_vol_spd and df_joined\n",
    "print(f\"Number of LINKID with enough data: {len(df_vol_spd['LINKID'].unique())}\")\n",
    "print(f\"Number of LINKID with enough data and Road Attributes: {len(df_joined['LINKID'].unique())}\")\n",
    "\n",
    "# Calculate unique LINKIDs that are in df_vol_spd but not in Seg_attr\n",
    "unique_in_df_vol_spd = set(df_vol_spd['LINKID'].unique()) - set(Seg_attr['LINKID'].unique())\n",
    "\n",
    "# Print the number of such unique LINKIDs\n",
    "print(f\"Number of LINKID with enough data but missing Road Attributes: {len(unique_in_df_vol_spd)}\")\n",
    "\n",
    "# Merge this information into summary_df\n",
    "summary_df = pd.merge(\n",
    "    summary_df,\n",
    "    pd.DataFrame({'missing_attribute_LINKID': list(unique_in_df_vol_spd)}),\n",
    "    how='left',\n",
    "    left_on=['Requested_LINKID'],\n",
    "    right_on=['missing_attribute_LINKID']\n",
    ")\n",
    "\n",
    "# Display the first few rows of the updated summary_df\n",
    "summary_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Free Flow Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LINKID</th>\n",
       "      <th>ROUTE_NAME</th>\n",
       "      <th>Lane_Capacity</th>\n",
       "      <th>LANE_NUM</th>\n",
       "      <th>POST_SPD</th>\n",
       "      <th>urban</th>\n",
       "      <th>Dir</th>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Year</th>\n",
       "      <th>Mon</th>\n",
       "      <th>Day</th>\n",
       "      <th>Wkday</th>\n",
       "      <th>TotalVol</th>\n",
       "      <th>AvgSpd</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>940761.0</td>\n",
       "      <td>R-VA   IS00295SB</td>\n",
       "      <td>2100</td>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>20170000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>66.578275</td>\n",
       "      <td>Vol Distribution&amp;INRIX Spd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>940761.0</td>\n",
       "      <td>R-VA   IS00295SB</td>\n",
       "      <td>2100</td>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>20170000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>66.415282</td>\n",
       "      <td>Vol Distribution&amp;INRIX Spd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>940761.0</td>\n",
       "      <td>R-VA   IS00295SB</td>\n",
       "      <td>2100</td>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>20170000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>66.052632</td>\n",
       "      <td>Vol Distribution&amp;INRIX Spd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>940761.0</td>\n",
       "      <td>R-VA   IS00295SB</td>\n",
       "      <td>2100</td>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>20170000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>66.101307</td>\n",
       "      <td>Vol Distribution&amp;INRIX Spd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>940761.0</td>\n",
       "      <td>R-VA   IS00295SB</td>\n",
       "      <td>2100</td>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>20170000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>66.236364</td>\n",
       "      <td>Vol Distribution&amp;INRIX Spd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID    LINKID        ROUTE_NAME  Lane_Capacity  LANE_NUM  POST_SPD  urban  \\\n",
       "0   1  940761.0  R-VA   IS00295SB           2100         3        70      0   \n",
       "1   1  940761.0  R-VA   IS00295SB           2100         3        70      0   \n",
       "2   1  940761.0  R-VA   IS00295SB           2100         3        70      0   \n",
       "3   1  940761.0  R-VA   IS00295SB           2100         3        70      0   \n",
       "4   1  940761.0  R-VA   IS00295SB           2100         3        70      0   \n",
       "\n",
       "  Dir        Date  Hour    Year   Mon   Day  Wkday  TotalVol     AvgSpd  \\\n",
       "0   S  20170000.0   0.0  2017.0  99.0  99.0   99.0     226.0  66.578275   \n",
       "1   S  20170000.0   1.0  2017.0  99.0  99.0   99.0     171.0  66.415282   \n",
       "2   S  20170000.0   2.0  2017.0  99.0  99.0   99.0     147.0  66.052632   \n",
       "3   S  20170000.0   3.0  2017.0  99.0  99.0   99.0     151.0  66.101307   \n",
       "4   S  20170000.0   4.0  2017.0  99.0  99.0   99.0     214.0  66.236364   \n",
       "\n",
       "                       Source  \n",
       "0  Vol Distribution&INRIX Spd  \n",
       "1  Vol Distribution&INRIX Spd  \n",
       "2  Vol Distribution&INRIX Spd  \n",
       "3  Vol Distribution&INRIX Spd  \n",
       "4  Vol Distribution&INRIX Spd  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Current data structure\n",
    "df_joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LINKID</th>\n",
       "      <th>FFSpd</th>\n",
       "      <th>FF_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>990120.0</td>\n",
       "      <td>62.817330</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>990112.0</td>\n",
       "      <td>58.094294</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>990108.0</td>\n",
       "      <td>58.771397</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>980503.0</td>\n",
       "      <td>71.928605</td>\n",
       "      <td>17190.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>980463.0</td>\n",
       "      <td>70.771581</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>980300.0</td>\n",
       "      <td>71.492136</td>\n",
       "      <td>8627.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>980296.0</td>\n",
       "      <td>67.107190</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>980254.0</td>\n",
       "      <td>70.635326</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>980188.0</td>\n",
       "      <td>72.076891</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>980011.0</td>\n",
       "      <td>71.022799</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     LINKID      FFSpd  FF_points\n",
       "0  990120.0  62.817330       32.0\n",
       "1  990112.0  58.094294       33.0\n",
       "2  990108.0  58.771397       46.0\n",
       "3  980503.0  71.928605    17190.0\n",
       "4  980463.0  70.771581       13.0\n",
       "5  980300.0  71.492136     8627.0\n",
       "6  980296.0  67.107190       56.0\n",
       "7  980254.0  70.635326       13.0\n",
       "8  980188.0  72.076891       52.0\n",
       "9  980011.0  71.022799       10.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create an empty DataFrame called 'df_FFSPD'\n",
    "df_FFSPD = pd.DataFrame()\n",
    "\n",
    "## Group the 'df_joined' DataFrame by 'LINKID' and filter for rows where 'AvgSpd' is greater than 'POST_SPD'\n",
    "grouped = df_joined.query('AvgSpd > POST_SPD').groupby('LINKID')\n",
    "\n",
    "## For each unique value of 'LINKID', calculate the average of the 'AvgSpd' column and create a new column called 'FFSpd' in the group DataFrame for speeds above 'POST_SPD'\n",
    "## Also create a new column called 'FF_points' in the group DataFrame for the number of points above 'POST_SPD'\n",
    "## Then, create a new DataFrame called 'new_row' that contains the LINKID, FFSpd, and FF_points for the group\n",
    "## Append 'new_row' to 'df_FFSPD' and reset the index of the resulting DataFrame\n",
    "for name, group in grouped:\n",
    "    mask = (group['TotalVol'] / group['LANE_NUM']) < 500\n",
    "    group['FFSpd'] = group['AvgSpd'][mask].mean()\n",
    "    group['FF_points'] = group['AvgSpd'][mask].count()\n",
    "    \n",
    "    new_row = pd.DataFrame({'LINKID': name, 'FFSpd':group['FFSpd'].mean(), 'FF_points':group['FF_points'].mean()}, index=[0])\n",
    "    df_FFSPD = pd.concat([new_row,df_FFSPD.loc[:]]).reset_index(drop=True)\n",
    "\n",
    "## Print the first 10 rows of the resulting DataFrame\n",
    "df_FFSPD.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge the 'df_joined' and 'df_FFSPD' DataFrames based on the 'LINKID' column and save the result to a new DataFrame called 'df_joined_FFSpd'\n",
    "df_joined_FFSpd = pd.merge(df_joined, df_FFSPD,  how='left', left_on=['LINKID'], right_on =['LINKID'])\n",
    "\n",
    "## Replace missing values in the 'FFSpd' column with the value of 'POST_SPD' + 7 where the 'FFSpd' column is null\n",
    "df_joined_FFSpd.loc[df_joined_FFSpd['FFSpd'].isnull(),'FFSpd'] = df_joined_FFSpd['POST_SPD'] + 7\n",
    "\n",
    "## Replace missing values in the 'FF_points' column with 0 where the 'FFSpd' column is null\n",
    "df_joined_FFSpd.loc[df_joined_FFSpd['FFSpd'].isnull(),'FF_points'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete the DataFrames to free up memory\n",
    "del grouped\n",
    "del df_joined\n",
    "\n",
    "# Call the garbage collector to free up memory\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area type lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of LINKID with Area Type info:  1231\n",
      "number of LINKID with enough data but missing Area Type:  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Requested_LINKID</th>\n",
       "      <th>missing_data_LINKID</th>\n",
       "      <th>Not_enough_data_LINKID</th>\n",
       "      <th>missing_attribute_LINKID</th>\n",
       "      <th>missing_Area_Type_LINKID_x</th>\n",
       "      <th>missing_Area_Type_LINKID_y</th>\n",
       "      <th>missing_Area_Type_LINKID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90112.0</td>\n",
       "      <td>90112.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90114.0</td>\n",
       "      <td>90114.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90116.0</td>\n",
       "      <td>90116.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90118.0</td>\n",
       "      <td>90118.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90120.0</td>\n",
       "      <td>90120.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Requested_LINKID  missing_data_LINKID  Not_enough_data_LINKID  \\\n",
       "0           90112.0              90112.0                     NaN   \n",
       "1           90114.0              90114.0                     NaN   \n",
       "2           90116.0              90116.0                     NaN   \n",
       "3           90118.0              90118.0                     NaN   \n",
       "4           90120.0              90120.0                     NaN   \n",
       "\n",
       "   missing_attribute_LINKID  missing_Area_Type_LINKID_x  \\\n",
       "0                       NaN                         NaN   \n",
       "1                       NaN                         NaN   \n",
       "2                       NaN                         NaN   \n",
       "3                       NaN                         NaN   \n",
       "4                       NaN                         NaN   \n",
       "\n",
       "   missing_Area_Type_LINKID_y  missing_Area_Type_LINKID  \n",
       "0                         NaN                       NaN  \n",
       "1                         NaN                       NaN  \n",
       "2                         NaN                       NaN  \n",
       "3                         NaN                       NaN  \n",
       "4                         NaN                       NaN  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename the 'urban' column to 'Area_type'\n",
    "df_joined_FFSpd.rename(columns={\"urban\": \"Area_type\"}, inplace=True)\n",
    "\n",
    "# Print the number of unique LINKIDs in df_joined_FFSpd\n",
    "print(\"number of LINKID with Area Type info: \", len(df_joined_FFSpd.LINKID.unique()))\n",
    "\n",
    "# Print the number of such unique LINKIDs\n",
    "unique_in_df_joined_FFSpd = set(df_joined_FFSpd.query('Area_type!=Area_type').LINKID.unique()) - set(df_joined_FFSpd.LINKID.unique())\n",
    "\n",
    "print(\"number of LINKID with enough data but missing Area Type: \", len(list(unique_in_df_joined_FFSpd)))\n",
    "\n",
    "# Merge this information into summary_df\n",
    "summary_df = pd.merge(summary_df, pd.DataFrame({'missing_Area_Type_LINKID': list(unique_in_df_joined_FFSpd)}),  how='left', left_on=['Requested_LINKID'], right_on =['missing_Area_Type_LINKID'])\n",
    "summary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Density based on Volume and Lane number\n",
    "df_joined_FFSpd['Density'] = (df_joined_FFSpd['TotalVol']/df_joined_FFSpd['LANE_NUM'])/df_joined_FFSpd['AvgSpd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup LOS result based on available density and area type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## These two lines filter out rows where the value in the \"Area_type\" column is missing (NaN).\n",
    "df_joined_FFSpd = df_joined_FFSpd.query('Area_type==Area_type')\n",
    "\n",
    "## These two lines filter out rows where the value in the \"AvgSpd\" column is missing (NaN).\n",
    "df_joined_FFSpd = df_joined_FFSpd.query('AvgSpd==AvgSpd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input shapefile after Peng's lane number update on March 5th, \n",
    "## contains 9 records Area type not 0 or 1 \n",
    "## currently assigning as 1 since they are all 1\n",
    "df_joined_FFSpd.loc[df_joined_FFSpd['Area_type'] > 1, 'Area_type'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the lookup tables as numpy arrays\n",
    "lookup_area_type_0 = np.array([[11,  'A'],\n",
    "                               [18, 'B'],\n",
    "                               [26, 'C'],\n",
    "                               [35, 'D'],\n",
    "                               [45,  'E'],\n",
    "                               [999, 'F']])\n",
    "lookup_area_type_1 = np.array([[6, 'A'],\n",
    "                               [14, 'B'],\n",
    "                               [22, 'C'],\n",
    "                               [29, 'D'],\n",
    "                               [39, 'E'],\n",
    "                               [999, 'F']])\n",
    "\n",
    "# Map area type values to lookup arrays\n",
    "lookup = {0: lookup_area_type_0, 1: lookup_area_type_1}\n",
    "\n",
    "def get_los(density, area_type):\n",
    "    # Get the lookup array for the given area type\n",
    "    look = lookup[area_type]\n",
    "    # Use numpy's `searchsorted` method to find the index of the first item in the array\n",
    "    # that is greater than or equal to the density\n",
    "    idx = np.searchsorted(look[:, 0].astype(float), density)\n",
    "    \n",
    "    # Get the LOS value based on the index\n",
    "    if idx< len(look):\n",
    "        los = look[idx, 1] \n",
    "    else: \n",
    "        los = look[idx-1, 1] \n",
    "    \n",
    "    return los\n",
    "\n",
    "\n",
    "df_joined_FFSpd['LOS'] = df_joined_FFSpd.apply(lambda row: get_los(row['Density'], row['Area_type']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined_FFSpd.to_csv(\"output\\\\output_With_spd.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.to_csv('output\\\\Current_Available_LINKID.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LINKID</th>\n",
       "      <th>ROUTE_NAME</th>\n",
       "      <th>Lane_Capacity</th>\n",
       "      <th>LANE_NUM</th>\n",
       "      <th>POST_SPD</th>\n",
       "      <th>Area_type</th>\n",
       "      <th>Dir</th>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Year</th>\n",
       "      <th>Mon</th>\n",
       "      <th>Day</th>\n",
       "      <th>Wkday</th>\n",
       "      <th>TotalVol</th>\n",
       "      <th>AvgSpd</th>\n",
       "      <th>Source</th>\n",
       "      <th>FFSpd</th>\n",
       "      <th>FF_points</th>\n",
       "      <th>Density</th>\n",
       "      <th>LOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10432448</th>\n",
       "      <td>697</td>\n",
       "      <td>40766.0</td>\n",
       "      <td>R-VA   IS00064EB</td>\n",
       "      <td>1800</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>20210101.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>534.0</td>\n",
       "      <td>64.794007</td>\n",
       "      <td>Speed Counter</td>\n",
       "      <td>62.874995</td>\n",
       "      <td>15879.0</td>\n",
       "      <td>2.747168</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10432472</th>\n",
       "      <td>697</td>\n",
       "      <td>40766.0</td>\n",
       "      <td>R-VA   IS00064EB</td>\n",
       "      <td>1800</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>20210102.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>810.0</td>\n",
       "      <td>65.246914</td>\n",
       "      <td>Speed Counter</td>\n",
       "      <td>62.874995</td>\n",
       "      <td>15879.0</td>\n",
       "      <td>4.138127</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10432496</th>\n",
       "      <td>697</td>\n",
       "      <td>40766.0</td>\n",
       "      <td>R-VA   IS00064EB</td>\n",
       "      <td>1800</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>20210103.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>63.473214</td>\n",
       "      <td>Speed Counter</td>\n",
       "      <td>62.874995</td>\n",
       "      <td>15879.0</td>\n",
       "      <td>2.940873</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10432520</th>\n",
       "      <td>697</td>\n",
       "      <td>40766.0</td>\n",
       "      <td>R-VA   IS00064EB</td>\n",
       "      <td>1800</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>20210104.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3422.0</td>\n",
       "      <td>64.544126</td>\n",
       "      <td>Speed Counter</td>\n",
       "      <td>62.874995</td>\n",
       "      <td>15879.0</td>\n",
       "      <td>17.672664</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10432544</th>\n",
       "      <td>697</td>\n",
       "      <td>40766.0</td>\n",
       "      <td>R-VA   IS00064EB</td>\n",
       "      <td>1800</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>20210105.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3552.0</td>\n",
       "      <td>64.455236</td>\n",
       "      <td>Speed Counter</td>\n",
       "      <td>62.874995</td>\n",
       "      <td>15879.0</td>\n",
       "      <td>18.369338</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID   LINKID        ROUTE_NAME  Lane_Capacity  LANE_NUM  POST_SPD  \\\n",
       "10432448  697  40766.0  R-VA   IS00064EB           1800         3        60   \n",
       "10432472  697  40766.0  R-VA   IS00064EB           1800         3        60   \n",
       "10432496  697  40766.0  R-VA   IS00064EB           1800         3        60   \n",
       "10432520  697  40766.0  R-VA   IS00064EB           1800         3        60   \n",
       "10432544  697  40766.0  R-VA   IS00064EB           1800         3        60   \n",
       "\n",
       "          Area_type Dir        Date  Hour    Year  Mon  Day  Wkday  TotalVol  \\\n",
       "10432448          1   E  20210101.0   7.0  2021.0  1.0  1.0    4.0     534.0   \n",
       "10432472          1   E  20210102.0   7.0  2021.0  1.0  2.0    5.0     810.0   \n",
       "10432496          1   E  20210103.0   7.0  2021.0  1.0  3.0    6.0     560.0   \n",
       "10432520          1   E  20210104.0   7.0  2021.0  1.0  4.0    0.0    3422.0   \n",
       "10432544          1   E  20210105.0   7.0  2021.0  1.0  5.0    1.0    3552.0   \n",
       "\n",
       "             AvgSpd         Source      FFSpd  FF_points    Density LOS  \n",
       "10432448  64.794007  Speed Counter  62.874995    15879.0   2.747168   A  \n",
       "10432472  65.246914  Speed Counter  62.874995    15879.0   4.138127   A  \n",
       "10432496  63.473214  Speed Counter  62.874995    15879.0   2.940873   A  \n",
       "10432520  64.544126  Speed Counter  62.874995    15879.0  17.672664   C  \n",
       "10432544  64.455236  Speed Counter  62.874995    15879.0  18.369338   C  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"examples for LOS calculation result\")\n",
    "df_joined_FFSpd.query('LINKID == 40766 and Year == 2021 and Hour == 7').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOS result reasonableness check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maybe double check these LINKID LANE NUMBER since the speed is above 60 and LOS is at E or F from lookup table\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([190077., 190073., 190075., 150318., 140056., 190078.,  50217.,\n",
       "       190074., 190071., 190174., 190066., 190072., 150066., 150069.,\n",
       "       840083., 140082., 140081., 150067., 140080., 150064., 150070.,\n",
       "       150051., 150065., 794135., 794132., 190012., 190013., 140236.,\n",
       "       190188., 140017., 190081., 140020., 140018., 160001., 140006.,\n",
       "       140019., 160004., 140012., 140243.,  50307.,  40777.,  50306.,\n",
       "        40108.,  50308.,  50155.,  40766.,  50305.,  50165.,  50169.,\n",
       "       150013.,  50598., 840070., 240500., 150041., 150024., 140051.,\n",
       "       794136., 199760., 150112.,  50204.,  50200., 150113., 190038.,\n",
       "       190032., 190035., 190030., 190040., 190041.,  20475.,  20176.,\n",
       "        10397.,  80373.,  90305.,  90206.,  40491., 140003.,  90204.,\n",
       "        50117.,  90200.,  40300.,  40298.,  40361.,  50587.,  40360.,\n",
       "        40304.,  60315., 190001., 190306., 190068., 190067.,  90272.,\n",
       "        90114.,  90112.,  90106.,  90118.,  90110.,  90116., 795420.,\n",
       "       789485., 795426.])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Maybe double check these LINKID LANE NUMBER since the speed is above 60 and LOS is at E or F from lookup table\")\n",
    "df_joined_FFSpd[['ID','LINKID', 'Year', 'Hour', 'AvgSpd', 'Density', 'TotalVol', 'LANE_NUM', 'Dir','LOS']].query('AvgSpd > 60 and LOS in [\"E\",\"F\"]').LINKID.unique()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
